
##Packages Used

library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(corrplot)

##Load and Assign Train and Test Data

trainUrl <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainFile <- "./data/pml-training.csv"
testFile  <- "./data/pml-testing.csv"
if (!file.exists("./data")) {dir.create("./data")}
if (!file.exists(trainFile)) {download.file(trainUrl, destfile=trainFile, method="curl")}
if (!file.exists(testFile)) {download.file(testUrl, destfile=testFile, method="curl")}


trainRaw <- read.csv("./data/pml-training.csv")
testRaw <- read.csv("./data/pml-testing.csv")

# Data validation checking for missing Data
sum(complete.cases(trainRaw))
# 406
dim(trainRaw)
# 19622   160

# Remove NA values
trainRaw <- trainRaw[, colSums(is.na(trainRaw)) == 0] 
testRaw <- testRaw[, colSums(is.na(testRaw)) == 0] 

# Remove Redundant variables 

classe <- trainRaw$classe
trainRemove <- grepl("^X|timestamp|window", names(trainRaw))
trainRaw <- trainRaw[, !trainRemove]
trainCleaned <- trainRaw[, sapply(trainRaw, is.numeric)]
trainCleaned$classe <- classe
testRemove <- grepl("^X|timestamp|window", names(testRaw))
testRaw <- testRaw[, !testRemove]
testCleaned <- testRaw[, sapply(testRaw, is.numeric)]

# Set seed for reproducibile results 

set.seed(22519) 

# Partition Train 70% and Test data 30% 

inTrain <- createDataPartition(trainCleaned$classe, p=0.70, list=F)
trainData <- trainCleaned[inTrain, ]
testData <- trainCleaned[-inTrain, ]

#Use Random Forest with 5 fold validation to model 

controlRf <- trainControl(method="cv", 5)
modelRf <- train(classe ~ ., data=trainData, method="rf", trControl=controlRf, ntree=250)
modelRf

## Random Forest OUTPUT
## 
## 13737 samples
##    52 predictor
##     5 classes: 'A', 'B', 'C', 'D', 'E' 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## 
## Summary of sample sizes: 10989, 10989, 10991, 10990, 10989 
## 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa      Accuracy SD   Kappa SD    
##    2    0.9904636  0.9879361  0.0006534224  0.0008263772
##   27    0.9913374  0.9890405  0.0015731292  0.0019917940
##   52    0.9850766  0.9811193  0.0027732182  0.0035098533
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was mtry = 27.

# Test model against test data 


#Use Confusion matrix to test model performance

predictRf <- predict(modelRf, testData)
confusionMatrix(testData$classe, predictRf)

## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    A    B    C    D    E
##          A 1672    1    0    0    1
##          B    8 1127    4    0    0
##          C    0    1 1020    5    0
##          D    0    0   14  949    1
##          E    0    0    0    6 1076
## 
## Overall Statistics
##                                          
##                Accuracy : 0.993          
##                  95% CI : (0.9906, 0.995)
##     No Information Rate : 0.2855         
##     P-Value [Acc > NIR] : < 2.2e-16      
##                                          
##                   Kappa : 0.9912         
##  Mcnemar's Test P-Value : NA             
## 
## Statistics by Class:
## 
##                      Class: A Class: B Class: C Class: D Class: E
## Sensitivity            0.9952   0.9982   0.9827   0.9885   0.9981
## Specificity            0.9995   0.9975   0.9988   0.9970   0.9988
## Pos Pred Value         0.9988   0.9895   0.9942   0.9844   0.9945
## Neg Pred Value         0.9981   0.9996   0.9963   0.9978   0.9996
## Prevalence             0.2855   0.1918   0.1764   0.1631   0.1832
## Detection Rate         0.2841   0.1915   0.1733   0.1613   0.1828
## Detection Prevalence   0.2845   0.1935   0.1743   0.1638   0.1839
## Balanced Accuracy      0.9974   0.9979   0.9907   0.9927   0.9984


accuracy <- postResample(predictRf, testData$classe)
accuracy

# Accuracy     Kappa 
# 0.9930331    0.9911870 


# Out of sample Error

oose <- 1 - as.numeric(confusionMatrix(testData$classe, predictRf)$overall[1])
oose

# Apply model to Original data
result <- predict(modelRf, testCleaned[, -length(names(testCleaned))])
result

## [1] B A B A A E D B A A B C B A E E A B B B
## Levels: A B C D E
